{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Data Exploration\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis (EDA) on the credit card fraud detection dataset to understand the data characteristics, distributions, and patterns that will inform our preprocessing and modeling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Starting Fraud Detection Data Exploration...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import os\n",
    "# Get the project root directory (parent of notebooks directory)\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(project_root, \"data\", \"raw\", \"creditcard.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\\nDescriptive Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis (Class Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the target variable\n",
    "class_counts = df['Class'].value_counts()\n",
    "class_percentages = df['Class'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(f\"Normal transactions: {class_counts[0]} ({class_percentages[0]:.2f}%)\")\n",
    "print(f\"Fraudulent transactions: {class_counts[1]} ({class_percentages[1]:.2f}%)\")\n",
    "print(f\"\\nClass imbalance ratio: {class_counts[0]/class_counts[1]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Count plot\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title('Transaction Class Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Class (0=Normal, 1=Fraud)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Normal', 'Fraud'])\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, count in enumerate(class_counts):\n",
    "    plt.text(i, count + 50, str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Subplot 2: Pie chart\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "plt.pie(class_percentages.values, labels=['Normal', 'Fraud'], colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Transaction Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Feature names: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Time and Amount features (the only non-PCA features)\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Time distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(df['Time'], bins=50, alpha=0.7, color='skyblue')\n",
    "plt.title('Time Distribution', fontweight='bold')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Amount distribution\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(df['Amount'], bins=50, alpha=0.7, color='lightgreen')\n",
    "plt.title('Amount Distribution', fontweight='bold')\n",
    "plt.xlabel('Amount ($)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Log scale for Amount to see the distribution better\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(np.log1p(df['Amount']), bins=50, alpha=0.7, color='lightgreen')\n",
    "plt.title('Amount Distribution (Log Scale)', fontweight='bold')\n",
    "plt.xlabel('Log(Amount + 1)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Time vs Amount scatter plot\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(df[df['Class']==0]['Time'], df[df['Class']==0]['Amount'], alpha=0.1, color='blue', label='Normal', s=1)\n",
    "plt.scatter(df[df['Class']==1]['Time'], df[df['Class']==1]['Amount'], alpha=0.5, color='red', label='Fraud', s=5)\n",
    "plt.title('Time vs Amount', fontweight='bold')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amount ($)')\n",
    "plt.legend()\n",
    "\n",
    "# Amount by class\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.boxplot([df[df['Class']==0]['Amount'], df[df['Class']==1]['Amount']], labels=['Normal', 'Fraud'])\n",
    "plt.title('Amount by Class', fontweight='bold')\n",
    "plt.ylabel('Amount ($)')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Time by class\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.boxplot([df[df['Class']==0]['Time'], df[df['Class']==1]['Time']], labels=['Normal', 'Fraud'])\n",
    "plt.title('Time by Class', fontweight='bold')\n",
    "plt.ylabel('Time (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA Features Analysis (V1-V28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze PCA features (V1-V28)\n",
    "pca_features = [f'V{i}' for i in range(1, 29)]\n",
    "\n",
    "print(\"PCA Features Statistics:\")\n",
    "print(df[pca_features].describe().T[['mean', 'std', 'min', 'max']].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of first 12 PCA features\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, feature in enumerate(pca_features[:12], 1):\n",
    "    plt.subplot(4, 3, i)\n",
    "    plt.hist(df[feature], bins=30, alpha=0.7, color='orange')\n",
    "    plt.title(f'{feature} Distribution', fontsize=10, fontweight='bold')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for all features\n",
    "plt.figure(figsize=(20, 16))\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=False, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Correlation Coefficient'})\n",
    "\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of features with the target variable\n",
    "correlations_with_target = df.corr()['Class'].sort_values(ascending=False)\n",
    "\n",
    "print(\"Features most correlated with fraud (Class=1):\")\n",
    "print(correlations_with_target.head(10))\n",
    "\n",
    "print(\"\\nFeatures most negatively correlated with fraud (Class=0):\")\n",
    "print(correlations_with_target.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations with target\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Remove the target variable itself from correlations\n",
    "target_correlations = correlations_with_target.drop('Class')\n",
    "\n",
    "# Plot top 10 most correlated features\n",
    "top_features = abs(target_correlations).sort_values(ascending=False).head(10)\n",
    "\n",
    "colors = ['red' if target_correlations[feat] < 0 else 'blue' for feat in top_features.index]\n",
    "\n",
    "plt.bar(range(len(top_features)), top_features.values, color=colors, alpha=0.7)\n",
    "plt.xticks(range(len(top_features)), top_features.index, rotation=45)\n",
    "plt.title('Top 10 Features by Absolute Correlation with Fraud', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Absolute Correlation Coefficient')\n",
    "plt.xlabel('Features')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation values on bars\n",
    "for i, (feature, value) in enumerate(zip(top_features.index, top_features.values)):\n",
    "    corr_value = target_correlations[feature]\n",
    "    plt.text(i, value + 0.01, f'{corr_value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Missing Values and Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"\\n‚úÖ No missing values found in the dataset\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {missing_values.sum()} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {duplicates} duplicate rows\")\n",
    "    print(\"Removing duplicates...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Dataset shape after removing duplicates: {df.shape}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No duplicate rows found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Distributions by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature distributions between normal and fraud transactions\n",
    "normal_data = df[df['Class'] == 0]\n",
    "fraud_data = df[df['Class'] == 1]\n",
    "\n",
    "print(f\"Normal transactions: {len(normal_data)}\")\n",
    "print(f\"Fraud transactions: {len(fraud_data)}\")\n",
    "\n",
    "# Distribution comparison for top correlated features\n",
    "top_correlated_features = abs(correlations_with_target).sort_values(ascending=False).head(6).index.tolist()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, feature in enumerate(top_correlated_features, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    \n",
    "    # Normal transactions\n",
    "    plt.hist(normal_data[feature], bins=30, alpha=0.6, label='Normal', color='blue', density=True)\n",
    "    \n",
    "    # Fraud transactions\n",
    "    plt.hist(fraud_data[feature], bins=30, alpha=0.6, label='Fraud', color='red', density=True)\n",
    "    \n",
    "    plt.title(f'{feature} Distribution by Class', fontweight='bold')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPLORATORY DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(f\"   - Total transactions: {len(df):,}\")\n",
    "print(f\"   - Features: {len(X.columns)}\")\n",
    "print(f\"   - Target classes: 2 (Normal=0, Fraud=1)\")\n",
    "\n",
    "print(f\"\\nüéØ CLASS IMBALANCE:\")\n",
    "print(f\"   - Normal transactions: {class_counts[0]:,} ({class_percentages[0]:.2f}%)\")\n",
    "print(f\"   - Fraud transactions: {class_counts[1]:,} ({class_percentages[1]:.2f}%)\")\n",
    "print(f\"   - Imbalance ratio: {class_counts[0]/class_counts[1]:.1f}:1\")\n",
    "print(f\"   ‚ö†Ô∏è  This is a highly imbalanced dataset!\")\n",
    "\n",
    "print(f\"\\nüí∞ AMOUNT ANALYSIS:\")\n",
    "print(f\"   - Mean amount (Normal): ${df[df['Class']==0]['Amount'].mean():.2f}\")\n",
    "print(f\"   - Mean amount (Fraud): ${df[df['Class']==1]['Amount'].mean():.2f}\")\n",
    "print(f\"   - Max amount: ${df['Amount'].max():.2f}\")\n",
    "print(f\"   - Min amount: ${df['Amount'].min():.2f}\")\n",
    "\n",
    "print(f\"\\n‚è∞ TIME ANALYSIS:\")\n",
    "print(f\"   - Time range: {df['Time'].min():.0f}s to {df['Time'].max():.0f}s\")\n",
    "print(f\"   - Total duration: {(df['Time'].max() - df['Time'].min())/3600:.1f} hours\")\n",
    "\n",
    "print(f\"\\nüîç FEATURE INSIGHTS:\")\n",
    "print(f\"   - PCA features (V1-V28): All standardized, mean ‚âà 0\")\n",
    "print(f\"   - Most correlated with fraud: {top_features.index[0]}\")\n",
    "print(f\"   - Strongest negative correlation: {target_correlations.idxmin()}\")\n",
    "\n",
    "print(f\"\\n‚úÖ DATA QUALITY:\")\n",
    "print(f\"   - Missing values: {missing_values.sum()}\")\n",
    "print(f\"   - Duplicate rows: {duplicates}\")\n",
    "print(f\"   - Data type consistency: All numeric\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATIONS FOR PREPROCESSING:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Handle class imbalance using:\")\n",
    "print(\"   - Undersampling majority class\")\n",
    "print(\"   - Oversampling minority class (SMOTE)\")\n",
    "print(\"   - Class weights in models\")\n",
    "print(\"\\n2. Feature scaling:\")\n",
    "print(\"   - Amount feature needs scaling (wide range)\")\n",
    "print(\"   - Time feature may benefit from scaling\")\n",
    "print(\"   - PCA features already standardized\")\n",
    "print(\"\\n3. Feature engineering opportunities:\")\n",
    "print(\"   - Time-based features (hour of day, day of week)\")\n",
    "print(\"   - Amount-based features (log transformation)\")\n",
    "print(\"   - Interaction features between top correlated variables\")\n",
    "print(\"\\n4. Model considerations:\")\n",
    "print(\"   - Use metrics suitable for imbalanced data (AUC-ROC, Precision-Recall)\")\n",
    "print(\"   - Focus on recall to minimize false negatives (missed fraud)\")\n",
    "print(\"   - Consider cost-sensitive learning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
